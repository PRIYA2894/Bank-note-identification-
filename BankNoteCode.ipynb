{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SjKdDCYwoND"
      },
      "source": [
        "#Importing important packages \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import keras # Neural Networks\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler # Z-scaling -- (column - columnmean)/standard deviation\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os \n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "ggBr9JrrxLK3",
        "outputId": "d792f502-9653-4488-a065-59aac8c286d3"
      },
      "source": [
        "data = pd.read_csv(\"BankNote_Authentication.csv\")\n",
        "data.head()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>variance</th>\n",
              "      <th>skewness</th>\n",
              "      <th>curtosis</th>\n",
              "      <th>entropy</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.62160</td>\n",
              "      <td>8.6661</td>\n",
              "      <td>-2.8073</td>\n",
              "      <td>-0.44699</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.54590</td>\n",
              "      <td>8.1674</td>\n",
              "      <td>-2.4586</td>\n",
              "      <td>-1.46210</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.86600</td>\n",
              "      <td>-2.6383</td>\n",
              "      <td>1.9242</td>\n",
              "      <td>0.10645</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.45660</td>\n",
              "      <td>9.5228</td>\n",
              "      <td>-4.0112</td>\n",
              "      <td>-3.59440</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.32924</td>\n",
              "      <td>-4.4552</td>\n",
              "      <td>4.5718</td>\n",
              "      <td>-0.98880</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   variance  skewness  curtosis  entropy  class\n",
              "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
              "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
              "2   3.86600   -2.6383    1.9242  0.10645      0\n",
              "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
              "4   0.32924   -4.4552    4.5718 -0.98880      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu90gbOExLTw",
        "outputId": "f30a950a-742e-404f-a2f5-8cf5c13a8dc8"
      },
      "source": [
        "data.isnull().sum() # checking the missing value "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "variance    0\n",
              "skewness    0\n",
              "curtosis    0\n",
              "entropy     0\n",
              "class       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih9pIgOcpzWo"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "imFwlR_8xLdT",
        "outputId": "8a3ef4f8-7dc8-4fc4-dec8-38977830fc2f"
      },
      "source": [
        "data.describe(include='all')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>variance</th>\n",
              "      <th>skewness</th>\n",
              "      <th>curtosis</th>\n",
              "      <th>entropy</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1372.000000</td>\n",
              "      <td>1372.000000</td>\n",
              "      <td>1372.000000</td>\n",
              "      <td>1372.000000</td>\n",
              "      <td>1372.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.433735</td>\n",
              "      <td>1.922353</td>\n",
              "      <td>1.397627</td>\n",
              "      <td>-1.191657</td>\n",
              "      <td>0.444606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.842763</td>\n",
              "      <td>5.869047</td>\n",
              "      <td>4.310030</td>\n",
              "      <td>2.101013</td>\n",
              "      <td>0.497103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-7.042100</td>\n",
              "      <td>-13.773100</td>\n",
              "      <td>-5.286100</td>\n",
              "      <td>-8.548200</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.773000</td>\n",
              "      <td>-1.708200</td>\n",
              "      <td>-1.574975</td>\n",
              "      <td>-2.413450</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.496180</td>\n",
              "      <td>2.319650</td>\n",
              "      <td>0.616630</td>\n",
              "      <td>-0.586650</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.821475</td>\n",
              "      <td>6.814625</td>\n",
              "      <td>3.179250</td>\n",
              "      <td>0.394810</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>6.824800</td>\n",
              "      <td>12.951600</td>\n",
              "      <td>17.927400</td>\n",
              "      <td>2.449500</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          variance     skewness     curtosis      entropy        class\n",
              "count  1372.000000  1372.000000  1372.000000  1372.000000  1372.000000\n",
              "mean      0.433735     1.922353     1.397627    -1.191657     0.444606\n",
              "std       2.842763     5.869047     4.310030     2.101013     0.497103\n",
              "min      -7.042100   -13.773100    -5.286100    -8.548200     0.000000\n",
              "25%      -1.773000    -1.708200    -1.574975    -2.413450     0.000000\n",
              "50%       0.496180     2.319650     0.616630    -0.586650     0.000000\n",
              "75%       2.821475     6.814625     3.179250     0.394810     1.000000\n",
              "max       6.824800    12.951600    17.927400     2.449500     1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "wJsyerMGxLke",
        "outputId": "36c2bb92-2e43-4ca1-a1c4-16f299502fcc"
      },
      "source": [
        "#dependant variable is extracted \n",
        "target=data['class']\n",
        "#independant variable is extracted \n",
        "data=data.drop(['class'],axis=1)\n",
        "data.head()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>variance</th>\n",
              "      <th>skewness</th>\n",
              "      <th>curtosis</th>\n",
              "      <th>entropy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.62160</td>\n",
              "      <td>8.6661</td>\n",
              "      <td>-2.8073</td>\n",
              "      <td>-0.44699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.54590</td>\n",
              "      <td>8.1674</td>\n",
              "      <td>-2.4586</td>\n",
              "      <td>-1.46210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.86600</td>\n",
              "      <td>-2.6383</td>\n",
              "      <td>1.9242</td>\n",
              "      <td>0.10645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.45660</td>\n",
              "      <td>9.5228</td>\n",
              "      <td>-4.0112</td>\n",
              "      <td>-3.59440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.32924</td>\n",
              "      <td>-4.4552</td>\n",
              "      <td>4.5718</td>\n",
              "      <td>-0.98880</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   variance  skewness  curtosis  entropy\n",
              "0   3.62160    8.6661   -2.8073 -0.44699\n",
              "1   4.54590    8.1674   -2.4586 -1.46210\n",
              "2   3.86600   -2.6383    1.9242  0.10645\n",
              "3   3.45660    9.5228   -4.0112 -3.59440\n",
              "4   0.32924   -4.4552    4.5718 -0.98880"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-_GIQOuxLpe"
      },
      "source": [
        "\n",
        "# ensure all data are floating point values\n",
        "data =data.astype('float32')\n",
        "# encode strings to integer\n",
        "target = LabelEncoder().fit_transform(target)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT0kYxubzE8a",
        "outputId": "c31c8ab6-d517-472b-face-5977ba1ac825"
      },
      "source": [
        "#divide dataset into training set and test set\n",
        "# and standardising dataset\n",
        "# 70% train, ,30% Test\n",
        "trainX, testX, trainY, testY = train_test_split(data, target, train_size=0.7, stratify=target, random_state=2)\n",
        "print(trainY.shape)\n",
        "print(np.sum(trainY))\n",
        "print(testY.shape)\n",
        "print(np.sum(testY))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(960,)\n",
            "427\n",
            "(412,)\n",
            "183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl8Gn_b_zFCl"
      },
      "source": [
        "scaler=StandardScaler().fit(trainX) # computes mean and stddev of each column of the train split\n",
        "trainX_scaled=scaler.transform(trainX) # operation - (column - its mean)/its stddev\n",
        "testX_scaled=scaler.transform(testX)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-dzz4SCzFNL"
      },
      "source": [
        "model=Sequential() # Name suggests that the layers of the Neural Network will be in sequence\n",
        "# Dense is just a name for a simple neural network layer\n",
        "\n",
        "#1st hidden layer\n",
        "model.add(Dense(15, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
        "              # 15 neurons, the first layer of a neural network must contain the input_dim which is the number of input data-columns\n",
        "              # Initially weights of the neural network are random values drawn from a standard gaussian/normal distribution between -1 to 1\n",
        "\n",
        "\n",
        "#Ouput layer(only 1 neuron because it is a binary classification problem, so we require only 1 output)\n",
        "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJRwgmPw0slk",
        "outputId": "404750cc-acfb-4b66-d4dd-c38ed9e079a5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 15)                75        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 91\n",
            "Trainable params: 91\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiXRjrZ10uRm"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Name of logistic loss in keras is binary_crossentropy\n",
        "# adam - adaptive learning rate, thatswhy no need to provide learning rate"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZM10dOn0udU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c178bf-23eb-459b-937e-756dfc02f291"
      },
      "source": [
        "# saving the model on every epoch \n",
        "if not os.path.exists('/content/saved_models'):\n",
        "    os.mkdir('/content/saved_models')\n",
        "\n",
        "filepath = '/content/saved_models/model-{epoch:05d}.h5'\n",
        "num_epochs = 250\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "callbacks_list = [checkpoint]\n",
        "history = model.fit(x=trainX_scaled, y=trainY, batch_size=50, epochs=num_epochs, verbose=1, callbacks=callbacks_list,\n",
        "                    validation_data=(testX_scaled, testY))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/250\n",
            "20/20 [==============================] - 1s 15ms/step - loss: 0.6908 - accuracy: 0.6815 - val_loss: 0.6846 - val_accuracy: 0.8374\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68464, saving model to /content/saved_models/model-00001.h5\n",
            "Epoch 2/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.6825 - accuracy: 0.8408 - val_loss: 0.6711 - val_accuracy: 0.8301\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.68464 to 0.67114, saving model to /content/saved_models/model-00002.h5\n",
            "Epoch 3/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.6682 - accuracy: 0.8244 - val_loss: 0.6492 - val_accuracy: 0.8398\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.67114 to 0.64921, saving model to /content/saved_models/model-00003.h5\n",
            "Epoch 4/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.6441 - accuracy: 0.8507 - val_loss: 0.6174 - val_accuracy: 0.8495\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.64921 to 0.61744, saving model to /content/saved_models/model-00004.h5\n",
            "Epoch 5/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.6116 - accuracy: 0.8617 - val_loss: 0.5771 - val_accuracy: 0.8786\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.61744 to 0.57709, saving model to /content/saved_models/model-00005.h5\n",
            "Epoch 6/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.5729 - accuracy: 0.8624 - val_loss: 0.5309 - val_accuracy: 0.8981\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.57709 to 0.53091, saving model to /content/saved_models/model-00006.h5\n",
            "Epoch 7/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5299 - accuracy: 0.8849 - val_loss: 0.4819 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.53091 to 0.48190, saving model to /content/saved_models/model-00007.h5\n",
            "Epoch 8/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4913 - accuracy: 0.8748 - val_loss: 0.4323 - val_accuracy: 0.9248\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.48190 to 0.43228, saving model to /content/saved_models/model-00008.h5\n",
            "Epoch 9/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4287 - accuracy: 0.9147 - val_loss: 0.3845 - val_accuracy: 0.9369\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.43228 to 0.38454, saving model to /content/saved_models/model-00009.h5\n",
            "Epoch 10/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3857 - accuracy: 0.9346 - val_loss: 0.3410 - val_accuracy: 0.9417\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.38454 to 0.34097, saving model to /content/saved_models/model-00010.h5\n",
            "Epoch 11/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3309 - accuracy: 0.9530 - val_loss: 0.3012 - val_accuracy: 0.9417\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.34097 to 0.30123, saving model to /content/saved_models/model-00011.h5\n",
            "Epoch 12/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2993 - accuracy: 0.9645 - val_loss: 0.2663 - val_accuracy: 0.9515\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.30123 to 0.26633, saving model to /content/saved_models/model-00012.h5\n",
            "Epoch 13/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.9580 - val_loss: 0.2365 - val_accuracy: 0.9563\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.26633 to 0.23649, saving model to /content/saved_models/model-00013.h5\n",
            "Epoch 14/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2336 - accuracy: 0.9669 - val_loss: 0.2107 - val_accuracy: 0.9660\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.23649 to 0.21073, saving model to /content/saved_models/model-00014.h5\n",
            "Epoch 15/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1972 - accuracy: 0.9701 - val_loss: 0.1887 - val_accuracy: 0.9709\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.21073 to 0.18867, saving model to /content/saved_models/model-00015.h5\n",
            "Epoch 16/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1824 - accuracy: 0.9692 - val_loss: 0.1703 - val_accuracy: 0.9660\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.18867 to 0.17030, saving model to /content/saved_models/model-00016.h5\n",
            "Epoch 17/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1669 - accuracy: 0.9697 - val_loss: 0.1538 - val_accuracy: 0.9709\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.17030 to 0.15379, saving model to /content/saved_models/model-00017.h5\n",
            "Epoch 18/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1482 - accuracy: 0.9768 - val_loss: 0.1406 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.15379 to 0.14064, saving model to /content/saved_models/model-00018.h5\n",
            "Epoch 19/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1396 - accuracy: 0.9706 - val_loss: 0.1297 - val_accuracy: 0.9757\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.14064 to 0.12969, saving model to /content/saved_models/model-00019.h5\n",
            "Epoch 20/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1161 - accuracy: 0.9841 - val_loss: 0.1202 - val_accuracy: 0.9757\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.12969 to 0.12020, saving model to /content/saved_models/model-00020.h5\n",
            "Epoch 21/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1187 - accuracy: 0.9718 - val_loss: 0.1118 - val_accuracy: 0.9757\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.12020 to 0.11182, saving model to /content/saved_models/model-00021.h5\n",
            "Epoch 22/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1080 - accuracy: 0.9776 - val_loss: 0.1046 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.11182 to 0.10461, saving model to /content/saved_models/model-00022.h5\n",
            "Epoch 23/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.9749 - val_loss: 0.0985 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.10461 to 0.09851, saving model to /content/saved_models/model-00023.h5\n",
            "Epoch 24/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0980 - accuracy: 0.9754 - val_loss: 0.0933 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.09851 to 0.09326, saving model to /content/saved_models/model-00024.h5\n",
            "Epoch 25/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0922 - accuracy: 0.9774 - val_loss: 0.0885 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.09326 to 0.08853, saving model to /content/saved_models/model-00025.h5\n",
            "Epoch 26/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0873 - accuracy: 0.9760 - val_loss: 0.0847 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.08853 to 0.08466, saving model to /content/saved_models/model-00026.h5\n",
            "Epoch 27/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0867 - accuracy: 0.9764 - val_loss: 0.0803 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.08466 to 0.08032, saving model to /content/saved_models/model-00027.h5\n",
            "Epoch 28/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0813 - accuracy: 0.9738 - val_loss: 0.0771 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.08032 to 0.07711, saving model to /content/saved_models/model-00028.h5\n",
            "Epoch 29/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.9812 - val_loss: 0.0740 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.07711 to 0.07396, saving model to /content/saved_models/model-00029.h5\n",
            "Epoch 30/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.9817 - val_loss: 0.0713 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.07396 to 0.07132, saving model to /content/saved_models/model-00030.h5\n",
            "Epoch 31/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.9741 - val_loss: 0.0687 - val_accuracy: 0.9806\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.07132 to 0.06872, saving model to /content/saved_models/model-00031.h5\n",
            "Epoch 32/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.9766 - val_loss: 0.0668 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.06872 to 0.06682, saving model to /content/saved_models/model-00032.h5\n",
            "Epoch 33/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9862 - val_loss: 0.0647 - val_accuracy: 0.9806\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.06682 to 0.06470, saving model to /content/saved_models/model-00033.h5\n",
            "Epoch 34/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.9786 - val_loss: 0.0633 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.06470 to 0.06334, saving model to /content/saved_models/model-00034.h5\n",
            "Epoch 35/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9780 - val_loss: 0.0615 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.06334 to 0.06147, saving model to /content/saved_models/model-00035.h5\n",
            "Epoch 36/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9881 - val_loss: 0.0592 - val_accuracy: 0.9806\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.06147 to 0.05916, saving model to /content/saved_models/model-00036.h5\n",
            "Epoch 37/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9886 - val_loss: 0.0576 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.05916 to 0.05759, saving model to /content/saved_models/model-00037.h5\n",
            "Epoch 38/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0529 - accuracy: 0.9820 - val_loss: 0.0566 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.05759 to 0.05661, saving model to /content/saved_models/model-00038.h5\n",
            "Epoch 39/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9830 - val_loss: 0.0554 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.05661 to 0.05535, saving model to /content/saved_models/model-00039.h5\n",
            "Epoch 40/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0549 - accuracy: 0.9778 - val_loss: 0.0543 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.05535 to 0.05432, saving model to /content/saved_models/model-00040.h5\n",
            "Epoch 41/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0557 - accuracy: 0.9788 - val_loss: 0.0531 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.05432 to 0.05310, saving model to /content/saved_models/model-00041.h5\n",
            "Epoch 42/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9790 - val_loss: 0.0517 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.05310 to 0.05166, saving model to /content/saved_models/model-00042.h5\n",
            "Epoch 43/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0441 - accuracy: 0.9831 - val_loss: 0.0507 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.05166 to 0.05067, saving model to /content/saved_models/model-00043.h5\n",
            "Epoch 44/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0494 - accuracy: 0.9802 - val_loss: 0.0501 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.05067 to 0.05015, saving model to /content/saved_models/model-00044.h5\n",
            "Epoch 45/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9783 - val_loss: 0.0494 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.05015 to 0.04942, saving model to /content/saved_models/model-00045.h5\n",
            "Epoch 46/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0483 - accuracy: 0.9810 - val_loss: 0.0487 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.04942 to 0.04874, saving model to /content/saved_models/model-00046.h5\n",
            "Epoch 47/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9774 - val_loss: 0.0476 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.04874 to 0.04758, saving model to /content/saved_models/model-00047.h5\n",
            "Epoch 48/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 0.9829 - val_loss: 0.0471 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.04758 to 0.04706, saving model to /content/saved_models/model-00048.h5\n",
            "Epoch 49/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.9823 - val_loss: 0.0465 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.04706 to 0.04649, saving model to /content/saved_models/model-00049.h5\n",
            "Epoch 50/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0393 - accuracy: 0.9842 - val_loss: 0.0458 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.04649 to 0.04585, saving model to /content/saved_models/model-00050.h5\n",
            "Epoch 51/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0477 - accuracy: 0.9771 - val_loss: 0.0451 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.04585 to 0.04513, saving model to /content/saved_models/model-00051.h5\n",
            "Epoch 52/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0433 - accuracy: 0.9789 - val_loss: 0.0444 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.04513 to 0.04441, saving model to /content/saved_models/model-00052.h5\n",
            "Epoch 53/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0408 - accuracy: 0.9803 - val_loss: 0.0438 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.04441 to 0.04382, saving model to /content/saved_models/model-00053.h5\n",
            "Epoch 54/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.9794 - val_loss: 0.0433 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.04382 to 0.04325, saving model to /content/saved_models/model-00054.h5\n",
            "Epoch 55/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0380 - accuracy: 0.9828 - val_loss: 0.0431 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.04325 to 0.04305, saving model to /content/saved_models/model-00055.h5\n",
            "Epoch 56/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0415 - accuracy: 0.9859 - val_loss: 0.0424 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.04305 to 0.04235, saving model to /content/saved_models/model-00056.h5\n",
            "Epoch 57/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0423 - accuracy: 0.9778 - val_loss: 0.0420 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.04235 to 0.04199, saving model to /content/saved_models/model-00057.h5\n",
            "Epoch 58/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0439 - accuracy: 0.9805 - val_loss: 0.0414 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.04199 to 0.04140, saving model to /content/saved_models/model-00058.h5\n",
            "Epoch 59/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0447 - accuracy: 0.9801 - val_loss: 0.0409 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.04140 to 0.04092, saving model to /content/saved_models/model-00059.h5\n",
            "Epoch 60/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.9788 - val_loss: 0.0404 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.04092 to 0.04045, saving model to /content/saved_models/model-00060.h5\n",
            "Epoch 61/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0360 - accuracy: 0.9861 - val_loss: 0.0398 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.04045 to 0.03984, saving model to /content/saved_models/model-00061.h5\n",
            "Epoch 62/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 0.9855 - val_loss: 0.0394 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.03984 to 0.03938, saving model to /content/saved_models/model-00062.h5\n",
            "Epoch 63/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0354 - accuracy: 0.9844 - val_loss: 0.0399 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.03938\n",
            "Epoch 64/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 0.9863 - val_loss: 0.0393 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.03938 to 0.03928, saving model to /content/saved_models/model-00064.h5\n",
            "Epoch 65/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 0.9850 - val_loss: 0.0386 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.03928 to 0.03856, saving model to /content/saved_models/model-00065.h5\n",
            "Epoch 66/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 0.9841 - val_loss: 0.0382 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.03856 to 0.03822, saving model to /content/saved_models/model-00066.h5\n",
            "Epoch 67/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.9835 - val_loss: 0.0381 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.03822 to 0.03813, saving model to /content/saved_models/model-00067.h5\n",
            "Epoch 68/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.9844 - val_loss: 0.0376 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.03813 to 0.03758, saving model to /content/saved_models/model-00068.h5\n",
            "Epoch 69/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0399 - accuracy: 0.9823 - val_loss: 0.0373 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.03758 to 0.03733, saving model to /content/saved_models/model-00069.h5\n",
            "Epoch 70/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0380 - accuracy: 0.9847 - val_loss: 0.0372 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.03733 to 0.03721, saving model to /content/saved_models/model-00070.h5\n",
            "Epoch 71/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 0.9837 - val_loss: 0.0373 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.03721\n",
            "Epoch 72/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0386 - accuracy: 0.9841 - val_loss: 0.0369 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.03721 to 0.03691, saving model to /content/saved_models/model-00072.h5\n",
            "Epoch 73/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.9867 - val_loss: 0.0361 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.03691 to 0.03614, saving model to /content/saved_models/model-00073.h5\n",
            "Epoch 74/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 0.9817 - val_loss: 0.0363 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.03614\n",
            "Epoch 75/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0383 - accuracy: 0.9792 - val_loss: 0.0357 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.03614 to 0.03567, saving model to /content/saved_models/model-00075.h5\n",
            "Epoch 76/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0391 - accuracy: 0.9790 - val_loss: 0.0353 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.03567 to 0.03530, saving model to /content/saved_models/model-00076.h5\n",
            "Epoch 77/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0418 - accuracy: 0.9768 - val_loss: 0.0351 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.03530 to 0.03510, saving model to /content/saved_models/model-00077.h5\n",
            "Epoch 78/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0304 - accuracy: 0.9860 - val_loss: 0.0343 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.03510 to 0.03435, saving model to /content/saved_models/model-00078.h5\n",
            "Epoch 79/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0349 - accuracy: 0.9819 - val_loss: 0.0344 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.03435\n",
            "Epoch 80/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.9855 - val_loss: 0.0344 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.03435\n",
            "Epoch 81/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.9818 - val_loss: 0.0344 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.03435\n",
            "Epoch 82/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.9905 - val_loss: 0.0348 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.03435\n",
            "Epoch 83/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0333 - accuracy: 0.9891 - val_loss: 0.0340 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.03435 to 0.03402, saving model to /content/saved_models/model-00083.h5\n",
            "Epoch 84/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0309 - accuracy: 0.9905 - val_loss: 0.0347 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.03402\n",
            "Epoch 85/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0315 - accuracy: 0.9897 - val_loss: 0.0342 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.03402\n",
            "Epoch 86/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0310 - accuracy: 0.9876 - val_loss: 0.0342 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.03402\n",
            "Epoch 87/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 0.9789 - val_loss: 0.0341 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.03402\n",
            "Epoch 88/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.9853 - val_loss: 0.0335 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.03402 to 0.03346, saving model to /content/saved_models/model-00088.h5\n",
            "Epoch 89/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.9893 - val_loss: 0.0331 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.03346 to 0.03314, saving model to /content/saved_models/model-00089.h5\n",
            "Epoch 90/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 0.9870 - val_loss: 0.0330 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.03314 to 0.03299, saving model to /content/saved_models/model-00090.h5\n",
            "Epoch 91/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.9942 - val_loss: 0.0326 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.03299 to 0.03261, saving model to /content/saved_models/model-00091.h5\n",
            "Epoch 92/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.9923 - val_loss: 0.0328 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.03261\n",
            "Epoch 93/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.9884 - val_loss: 0.0328 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.03261\n",
            "Epoch 94/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.9870 - val_loss: 0.0325 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.03261 to 0.03249, saving model to /content/saved_models/model-00094.h5\n",
            "Epoch 95/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0294 - accuracy: 0.9922 - val_loss: 0.0324 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.03249 to 0.03238, saving model to /content/saved_models/model-00095.h5\n",
            "Epoch 96/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9888 - val_loss: 0.0324 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.03238 to 0.03238, saving model to /content/saved_models/model-00096.h5\n",
            "Epoch 97/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0310 - accuracy: 0.9867 - val_loss: 0.0329 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.03238\n",
            "Epoch 98/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0293 - accuracy: 0.9880 - val_loss: 0.0325 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.03238\n",
            "Epoch 99/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.9942 - val_loss: 0.0322 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.03238 to 0.03219, saving model to /content/saved_models/model-00099.h5\n",
            "Epoch 100/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.9921 - val_loss: 0.0324 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.03219\n",
            "Epoch 101/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.9886 - val_loss: 0.0322 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.03219\n",
            "Epoch 102/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0276 - accuracy: 0.9875 - val_loss: 0.0313 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.03219 to 0.03128, saving model to /content/saved_models/model-00102.h5\n",
            "Epoch 103/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.9878 - val_loss: 0.0313 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.03128\n",
            "Epoch 104/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.9878 - val_loss: 0.0315 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.03128\n",
            "Epoch 105/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.9903 - val_loss: 0.0315 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.03128\n",
            "Epoch 106/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.9876 - val_loss: 0.0313 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.03128\n",
            "Epoch 107/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0299 - accuracy: 0.9866 - val_loss: 0.0312 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.03128 to 0.03125, saving model to /content/saved_models/model-00107.h5\n",
            "Epoch 108/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.9878 - val_loss: 0.0312 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.03125 to 0.03119, saving model to /content/saved_models/model-00108.h5\n",
            "Epoch 109/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.9883 - val_loss: 0.0308 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.03119 to 0.03079, saving model to /content/saved_models/model-00109.h5\n",
            "Epoch 110/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.9947 - val_loss: 0.0302 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.03079 to 0.03023, saving model to /content/saved_models/model-00110.h5\n",
            "Epoch 111/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.9888 - val_loss: 0.0307 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.03023\n",
            "Epoch 112/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0280 - accuracy: 0.9882 - val_loss: 0.0307 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.03023\n",
            "Epoch 113/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.9929 - val_loss: 0.0305 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.03023\n",
            "Epoch 114/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0295 - accuracy: 0.9841 - val_loss: 0.0305 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.03023\n",
            "Epoch 115/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 0.9876 - val_loss: 0.0302 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.03023 to 0.03021, saving model to /content/saved_models/model-00115.h5\n",
            "Epoch 116/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.9865 - val_loss: 0.0302 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.03021 to 0.03018, saving model to /content/saved_models/model-00116.h5\n",
            "Epoch 117/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 0.9874 - val_loss: 0.0302 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.03018\n",
            "Epoch 118/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9850 - val_loss: 0.0302 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.03018\n",
            "Epoch 119/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.9944 - val_loss: 0.0297 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.03018 to 0.02970, saving model to /content/saved_models/model-00119.h5\n",
            "Epoch 120/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.9879 - val_loss: 0.0303 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.02970\n",
            "Epoch 121/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.9877 - val_loss: 0.0299 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.02970\n",
            "Epoch 122/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.9914 - val_loss: 0.0298 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.02970\n",
            "Epoch 123/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.9909 - val_loss: 0.0294 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.02970 to 0.02943, saving model to /content/saved_models/model-00123.h5\n",
            "Epoch 124/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0288 - accuracy: 0.9895 - val_loss: 0.0291 - val_accuracy: 0.9903\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.02943 to 0.02912, saving model to /content/saved_models/model-00124.h5\n",
            "Epoch 125/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.9892 - val_loss: 0.0294 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.02912\n",
            "Epoch 126/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0224 - accuracy: 0.9935 - val_loss: 0.0292 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.02912\n",
            "Epoch 127/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.9916 - val_loss: 0.0292 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.02912\n",
            "Epoch 128/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.9873 - val_loss: 0.0293 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.02912\n",
            "Epoch 129/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.9903 - val_loss: 0.0292 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.02912\n",
            "Epoch 130/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.9928 - val_loss: 0.0294 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.02912\n",
            "Epoch 131/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0317 - accuracy: 0.9866 - val_loss: 0.0292 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.02912\n",
            "Epoch 132/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.9916 - val_loss: 0.0292 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.02912\n",
            "Epoch 133/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.9860 - val_loss: 0.0293 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.02912\n",
            "Epoch 134/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.9933 - val_loss: 0.0291 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00134: val_loss improved from 0.02912 to 0.02907, saving model to /content/saved_models/model-00134.h5\n",
            "Epoch 135/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.9923 - val_loss: 0.0285 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.02907 to 0.02852, saving model to /content/saved_models/model-00135.h5\n",
            "Epoch 136/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.9908 - val_loss: 0.0299 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.02852\n",
            "Epoch 137/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.9903 - val_loss: 0.0293 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.02852\n",
            "Epoch 138/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.9886 - val_loss: 0.0285 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00138: val_loss improved from 0.02852 to 0.02846, saving model to /content/saved_models/model-00138.h5\n",
            "Epoch 139/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.9898 - val_loss: 0.0287 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.02846\n",
            "Epoch 140/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.9893 - val_loss: 0.0290 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.02846\n",
            "Epoch 141/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.9890 - val_loss: 0.0295 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.02846\n",
            "Epoch 142/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.9905 - val_loss: 0.0293 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.02846\n",
            "Epoch 143/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0325 - accuracy: 0.9832 - val_loss: 0.0304 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.02846\n",
            "Epoch 144/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.9921 - val_loss: 0.0296 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.02846\n",
            "Epoch 145/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.9860 - val_loss: 0.0291 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.02846\n",
            "Epoch 146/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.9909 - val_loss: 0.0290 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.02846\n",
            "Epoch 147/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.9912 - val_loss: 0.0290 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.02846\n",
            "Epoch 148/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.9893 - val_loss: 0.0296 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.02846\n",
            "Epoch 149/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.9905 - val_loss: 0.0282 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00149: val_loss improved from 0.02846 to 0.02817, saving model to /content/saved_models/model-00149.h5\n",
            "Epoch 150/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.9896 - val_loss: 0.0295 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.02817\n",
            "Epoch 151/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.9918 - val_loss: 0.0288 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.02817\n",
            "Epoch 152/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.9886 - val_loss: 0.0293 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.02817\n",
            "Epoch 153/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.9835 - val_loss: 0.0287 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.02817\n",
            "Epoch 154/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.9881 - val_loss: 0.0286 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.02817\n",
            "Epoch 155/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.9889 - val_loss: 0.0283 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.02817\n",
            "Epoch 156/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.9872 - val_loss: 0.0282 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.02817\n",
            "Epoch 157/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.9899 - val_loss: 0.0282 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00157: val_loss improved from 0.02817 to 0.02817, saving model to /content/saved_models/model-00157.h5\n",
            "Epoch 158/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.9914 - val_loss: 0.0286 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.02817\n",
            "Epoch 159/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.9898 - val_loss: 0.0281 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00159: val_loss improved from 0.02817 to 0.02811, saving model to /content/saved_models/model-00159.h5\n",
            "Epoch 160/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.9916 - val_loss: 0.0274 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.02811 to 0.02745, saving model to /content/saved_models/model-00160.h5\n",
            "Epoch 161/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.9884 - val_loss: 0.0293 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.02745\n",
            "Epoch 162/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.9924 - val_loss: 0.0287 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.02745\n",
            "Epoch 163/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 0.9913 - val_loss: 0.0285 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.02745\n",
            "Epoch 164/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.9946 - val_loss: 0.0291 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.02745\n",
            "Epoch 165/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9946 - val_loss: 0.0289 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.02745\n",
            "Epoch 166/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.9955 - val_loss: 0.0285 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.02745\n",
            "Epoch 167/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.9931 - val_loss: 0.0285 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.02745\n",
            "Epoch 168/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.9932 - val_loss: 0.0284 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.02745\n",
            "Epoch 169/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.9901 - val_loss: 0.0282 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.02745\n",
            "Epoch 170/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.9897 - val_loss: 0.0282 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.02745\n",
            "Epoch 171/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.9928 - val_loss: 0.0280 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.02745\n",
            "Epoch 172/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.9902 - val_loss: 0.0283 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.02745\n",
            "Epoch 173/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.9893 - val_loss: 0.0284 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.02745\n",
            "Epoch 174/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.9932 - val_loss: 0.0280 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.02745\n",
            "Epoch 175/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.9867 - val_loss: 0.0282 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.02745\n",
            "Epoch 176/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.9944 - val_loss: 0.0277 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.02745\n",
            "Epoch 177/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.9864 - val_loss: 0.0280 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.02745\n",
            "Epoch 178/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.9940 - val_loss: 0.0278 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.02745\n",
            "Epoch 179/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.9920 - val_loss: 0.0279 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.02745\n",
            "Epoch 180/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.9861 - val_loss: 0.0280 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.02745\n",
            "Epoch 181/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0191 - accuracy: 0.9931 - val_loss: 0.0275 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.02745\n",
            "Epoch 182/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.9875 - val_loss: 0.0279 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.02745\n",
            "Epoch 183/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.9967 - val_loss: 0.0276 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.02745\n",
            "Epoch 184/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.9904 - val_loss: 0.0278 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.02745\n",
            "Epoch 185/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.9915 - val_loss: 0.0284 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.02745\n",
            "Epoch 186/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.9932 - val_loss: 0.0277 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.02745\n",
            "Epoch 187/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.9919 - val_loss: 0.0277 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.02745\n",
            "Epoch 188/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0171 - accuracy: 0.9956 - val_loss: 0.0278 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.02745\n",
            "Epoch 189/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.9933 - val_loss: 0.0277 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.02745\n",
            "Epoch 190/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0152 - accuracy: 0.9959 - val_loss: 0.0292 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.02745\n",
            "Epoch 191/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.9929 - val_loss: 0.0279 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.02745\n",
            "Epoch 192/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.9906 - val_loss: 0.0277 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.02745\n",
            "Epoch 193/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.9931 - val_loss: 0.0279 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.02745\n",
            "Epoch 194/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.9956 - val_loss: 0.0276 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.02745\n",
            "Epoch 195/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.9928 - val_loss: 0.0279 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.02745\n",
            "Epoch 196/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0164 - accuracy: 0.9932 - val_loss: 0.0267 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00196: val_loss improved from 0.02745 to 0.02666, saving model to /content/saved_models/model-00196.h5\n",
            "Epoch 197/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.9897 - val_loss: 0.0281 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.02666\n",
            "Epoch 198/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0191 - accuracy: 0.9935 - val_loss: 0.0285 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.02666\n",
            "Epoch 199/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.9894 - val_loss: 0.0278 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.02666\n",
            "Epoch 200/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.9905 - val_loss: 0.0276 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.02666\n",
            "Epoch 201/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.9944 - val_loss: 0.0278 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.02666\n",
            "Epoch 202/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.9903 - val_loss: 0.0278 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.02666\n",
            "Epoch 203/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.9937 - val_loss: 0.0277 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.02666\n",
            "Epoch 204/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0182 - accuracy: 0.9924 - val_loss: 0.0271 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.02666\n",
            "Epoch 205/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.9942 - val_loss: 0.0275 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.02666\n",
            "Epoch 206/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0170 - accuracy: 0.9921 - val_loss: 0.0274 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.02666\n",
            "Epoch 207/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.9946 - val_loss: 0.0279 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.02666\n",
            "Epoch 208/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.0275 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.02666\n",
            "Epoch 209/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.9954 - val_loss: 0.0275 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.02666\n",
            "Epoch 210/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.9938 - val_loss: 0.0276 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.02666\n",
            "Epoch 211/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.9945 - val_loss: 0.0274 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.02666\n",
            "Epoch 212/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 0.9961 - val_loss: 0.0286 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.02666\n",
            "Epoch 213/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.9946 - val_loss: 0.0278 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.02666\n",
            "Epoch 214/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 0.9958 - val_loss: 0.0290 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.02666\n",
            "Epoch 215/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.9958 - val_loss: 0.0296 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.02666\n",
            "Epoch 216/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.0283 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.02666\n",
            "Epoch 217/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.9907 - val_loss: 0.0271 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.02666\n",
            "Epoch 218/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.9896 - val_loss: 0.0288 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.02666\n",
            "Epoch 219/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0239 - accuracy: 0.9868 - val_loss: 0.0294 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.02666\n",
            "Epoch 220/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.9866 - val_loss: 0.0288 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.02666\n",
            "Epoch 221/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0151 - accuracy: 0.9934 - val_loss: 0.0281 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.02666\n",
            "Epoch 222/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.9905 - val_loss: 0.0280 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.02666\n",
            "Epoch 223/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0152 - accuracy: 0.9961 - val_loss: 0.0280 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.02666\n",
            "Epoch 224/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0152 - accuracy: 0.9950 - val_loss: 0.0270 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.02666\n",
            "Epoch 225/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.9912 - val_loss: 0.0282 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.02666\n",
            "Epoch 226/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.9914 - val_loss: 0.0276 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.02666\n",
            "Epoch 227/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 0.9918 - val_loss: 0.0279 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.02666\n",
            "Epoch 228/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 0.9924 - val_loss: 0.0274 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.02666\n",
            "Epoch 229/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.9916 - val_loss: 0.0277 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.02666\n",
            "Epoch 230/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 0.9961 - val_loss: 0.0273 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.02666\n",
            "Epoch 231/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0169 - accuracy: 0.9956 - val_loss: 0.0281 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.02666\n",
            "Epoch 232/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.9923 - val_loss: 0.0279 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.02666\n",
            "Epoch 233/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9966 - val_loss: 0.0279 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.02666\n",
            "Epoch 234/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.9892 - val_loss: 0.0294 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.02666\n",
            "Epoch 235/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.9913 - val_loss: 0.0283 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.02666\n",
            "Epoch 236/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9957 - val_loss: 0.0278 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.02666\n",
            "Epoch 237/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.9912 - val_loss: 0.0280 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.02666\n",
            "Epoch 238/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.9909 - val_loss: 0.0281 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.02666\n",
            "Epoch 239/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.9927 - val_loss: 0.0273 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.02666\n",
            "Epoch 240/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.9884 - val_loss: 0.0274 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.02666\n",
            "Epoch 241/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.9927 - val_loss: 0.0271 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.02666\n",
            "Epoch 242/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0157 - accuracy: 0.9944 - val_loss: 0.0272 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.02666\n",
            "Epoch 243/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.9902 - val_loss: 0.0276 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.02666\n",
            "Epoch 244/250\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0168 - accuracy: 0.9936 - val_loss: 0.0276 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.02666\n",
            "Epoch 245/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.9938 - val_loss: 0.0273 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.02666\n",
            "Epoch 246/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0163 - accuracy: 0.9946 - val_loss: 0.0274 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.02666\n",
            "Epoch 247/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.9919 - val_loss: 0.0278 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.02666\n",
            "Epoch 248/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.0273 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.02666\n",
            "Epoch 249/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.9898 - val_loss: 0.0279 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.02666\n",
            "Epoch 250/250\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.9951 - val_loss: 0.0268 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.02666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPEF0YW_0ujK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c02eb07-b33f-46bc-ae3a-7e18b961de88"
      },
      "source": [
        "history.history # Python Dictionary with 4 keys accuracy, loss (training), val_loss, val_accuracy"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': [0.7437499761581421,\n",
              "  0.8302083611488342,\n",
              "  0.8333333134651184,\n",
              "  0.8489583134651184,\n",
              "  0.8635416626930237,\n",
              "  0.878125011920929,\n",
              "  0.8885416388511658,\n",
              "  0.9041666388511658,\n",
              "  0.921875,\n",
              "  0.9416666626930237,\n",
              "  0.9520833492279053,\n",
              "  0.9583333134651184,\n",
              "  0.9624999761581421,\n",
              "  0.965624988079071,\n",
              "  0.9677083492279053,\n",
              "  0.9708333611488342,\n",
              "  0.9729166626930237,\n",
              "  0.9739583134651184,\n",
              "  0.9750000238418579,\n",
              "  0.9770833253860474,\n",
              "  0.9770833253860474,\n",
              "  0.9781249761581421,\n",
              "  0.9781249761581421,\n",
              "  0.9781249761581421,\n",
              "  0.9781249761581421,\n",
              "  0.9781249761581421,\n",
              "  0.9781249761581421,\n",
              "  0.9781249761581421,\n",
              "  0.9781249761581421,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9791666865348816,\n",
              "  0.9791666865348816,\n",
              "  0.9791666865348816,\n",
              "  0.9781249761581421,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9802083373069763,\n",
              "  0.9833333492279053,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.984375,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9906250238418579,\n",
              "  0.9906250238418579,\n",
              "  0.9906250238418579,\n",
              "  0.9906250238418579,\n",
              "  0.9895833134651184,\n",
              "  0.9885416626930237,\n",
              "  0.9906250238418579,\n",
              "  0.9906250238418579,\n",
              "  0.9906250238418579,\n",
              "  0.9906250238418579,\n",
              "  0.9906250238418579,\n",
              "  0.9906250238418579,\n",
              "  0.9906250238418579,\n",
              "  0.9906250238418579,\n",
              "  0.9906250238418579,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9895833134651184,\n",
              "  0.9885416626930237,\n",
              "  0.9895833134651184,\n",
              "  0.9916666746139526,\n",
              "  0.9895833134651184,\n",
              "  0.9895833134651184,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9895833134651184,\n",
              "  0.9906250238418579,\n",
              "  0.9895833134651184,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9895833134651184,\n",
              "  0.9895833134651184,\n",
              "  0.9895833134651184,\n",
              "  0.9895833134651184,\n",
              "  0.9895833134651184,\n",
              "  0.9885416626930237,\n",
              "  0.9885416626930237,\n",
              "  0.9927083253860474,\n",
              "  0.9885416626930237,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9906250238418579,\n",
              "  0.9916666746139526,\n",
              "  0.9895833134651184,\n",
              "  0.9895833134651184,\n",
              "  0.9916666746139526,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9895833134651184,\n",
              "  0.9895833134651184,\n",
              "  0.9906250238418579,\n",
              "  0.9927083253860474,\n",
              "  0.9895833134651184,\n",
              "  0.9895833134651184,\n",
              "  0.9895833134651184,\n",
              "  0.9906250238418579,\n",
              "  0.9916666746139526,\n",
              "  0.9895833134651184,\n",
              "  0.9937499761581421,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9895833134651184,\n",
              "  0.9937499761581421,\n",
              "  0.9937499761581421,\n",
              "  0.9906250238418579,\n",
              "  0.9937499761581421,\n",
              "  0.9937499761581421,\n",
              "  0.9937499761581421,\n",
              "  0.9885416626930237,\n",
              "  0.9906250238418579,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9937499761581421,\n",
              "  0.9937499761581421,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9937499761581421,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9895833134651184,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474,\n",
              "  0.9927083253860474],\n",
              " 'loss': [0.6894370913505554,\n",
              "  0.6801584362983704,\n",
              "  0.6636216640472412,\n",
              "  0.6384182572364807,\n",
              "  0.6040469408035278,\n",
              "  0.5620108246803284,\n",
              "  0.5151144862174988,\n",
              "  0.4655763506889343,\n",
              "  0.41676247119903564,\n",
              "  0.3702205717563629,\n",
              "  0.32695895433425903,\n",
              "  0.28822994232177734,\n",
              "  0.2544002830982208,\n",
              "  0.2252013087272644,\n",
              "  0.20016367733478546,\n",
              "  0.17902284860610962,\n",
              "  0.16113924980163574,\n",
              "  0.14617401361465454,\n",
              "  0.13352805376052856,\n",
              "  0.12295312434434891,\n",
              "  0.11378971487283707,\n",
              "  0.10600989311933517,\n",
              "  0.09947771579027176,\n",
              "  0.09370294958353043,\n",
              "  0.0886688083410263,\n",
              "  0.08418915420770645,\n",
              "  0.080159492790699,\n",
              "  0.07672347128391266,\n",
              "  0.07352786511182785,\n",
              "  0.07079832255840302,\n",
              "  0.0682358592748642,\n",
              "  0.06586458534002304,\n",
              "  0.0638078823685646,\n",
              "  0.06196214258670807,\n",
              "  0.060223400592803955,\n",
              "  0.05862359702587128,\n",
              "  0.05720460042357445,\n",
              "  0.05566644296050072,\n",
              "  0.054447393864393234,\n",
              "  0.05322342738509178,\n",
              "  0.05209975689649582,\n",
              "  0.050974585115909576,\n",
              "  0.050044432282447815,\n",
              "  0.049158234149217606,\n",
              "  0.04822715371847153,\n",
              "  0.04745927080512047,\n",
              "  0.04677233844995499,\n",
              "  0.04596244916319847,\n",
              "  0.045167215168476105,\n",
              "  0.04452009126543999,\n",
              "  0.0438842810690403,\n",
              "  0.04328515753149986,\n",
              "  0.042726531624794006,\n",
              "  0.04210519418120384,\n",
              "  0.04164603725075722,\n",
              "  0.04113195464015007,\n",
              "  0.040570616722106934,\n",
              "  0.040069930255413055,\n",
              "  0.03969334065914154,\n",
              "  0.039257489144802094,\n",
              "  0.038773491978645325,\n",
              "  0.03835580125451088,\n",
              "  0.03795275837182999,\n",
              "  0.037569720298051834,\n",
              "  0.03719988837838173,\n",
              "  0.03683721274137497,\n",
              "  0.03647404909133911,\n",
              "  0.036153148859739304,\n",
              "  0.03583601862192154,\n",
              "  0.03546163812279701,\n",
              "  0.03531064838171005,\n",
              "  0.03482109308242798,\n",
              "  0.034670811146497726,\n",
              "  0.03429550305008888,\n",
              "  0.03390342742204666,\n",
              "  0.0336691215634346,\n",
              "  0.03345769643783569,\n",
              "  0.03321780636906624,\n",
              "  0.032973505556583405,\n",
              "  0.032673198729753494,\n",
              "  0.03241673856973648,\n",
              "  0.03220371529459953,\n",
              "  0.031962692737579346,\n",
              "  0.0316515751183033,\n",
              "  0.03144102916121483,\n",
              "  0.03130213916301727,\n",
              "  0.031163331121206284,\n",
              "  0.030762959271669388,\n",
              "  0.030552854761481285,\n",
              "  0.030313650146126747,\n",
              "  0.03024301864206791,\n",
              "  0.02991488017141819,\n",
              "  0.029796291142702103,\n",
              "  0.029583744704723358,\n",
              "  0.02951560728251934,\n",
              "  0.029375402256846428,\n",
              "  0.02905881218612194,\n",
              "  0.028917618095874786,\n",
              "  0.028778428211808205,\n",
              "  0.028593841940164566,\n",
              "  0.02855897881090641,\n",
              "  0.02839278243482113,\n",
              "  0.028395574539899826,\n",
              "  0.028043294325470924,\n",
              "  0.027797620743513107,\n",
              "  0.02772085927426815,\n",
              "  0.027534244582057,\n",
              "  0.027419550344347954,\n",
              "  0.02724798582494259,\n",
              "  0.02730376087129116,\n",
              "  0.026957839727401733,\n",
              "  0.026957543566823006,\n",
              "  0.026740146800875664,\n",
              "  0.026607466861605644,\n",
              "  0.02647155523300171,\n",
              "  0.026525650173425674,\n",
              "  0.026184838265180588,\n",
              "  0.026164107024669647,\n",
              "  0.026176312938332558,\n",
              "  0.025956949219107628,\n",
              "  0.025728220120072365,\n",
              "  0.025782054290175438,\n",
              "  0.025464121252298355,\n",
              "  0.02551034837961197,\n",
              "  0.025384603068232536,\n",
              "  0.025236662477254868,\n",
              "  0.0251607783138752,\n",
              "  0.02499767765402794,\n",
              "  0.024980027228593826,\n",
              "  0.024931812658905983,\n",
              "  0.02475862018764019,\n",
              "  0.02461155131459236,\n",
              "  0.02451016753911972,\n",
              "  0.024445120245218277,\n",
              "  0.02446810156106949,\n",
              "  0.024345945566892624,\n",
              "  0.024174464866518974,\n",
              "  0.024285143241286278,\n",
              "  0.02412068098783493,\n",
              "  0.023865289986133575,\n",
              "  0.023832589387893677,\n",
              "  0.023756805807352066,\n",
              "  0.023719660937786102,\n",
              "  0.023500429466366768,\n",
              "  0.023369790986180305,\n",
              "  0.023464452475309372,\n",
              "  0.02327844128012657,\n",
              "  0.023360397666692734,\n",
              "  0.023185761645436287,\n",
              "  0.023112840950489044,\n",
              "  0.022983521223068237,\n",
              "  0.022922439500689507,\n",
              "  0.022731326520442963,\n",
              "  0.022765157744288445,\n",
              "  0.022557536140084267,\n",
              "  0.022540783509612083,\n",
              "  0.022422244772315025,\n",
              "  0.02242175117135048,\n",
              "  0.022379448637366295,\n",
              "  0.0225155521184206,\n",
              "  0.02255099266767502,\n",
              "  0.022127028554677963,\n",
              "  0.02205292321741581,\n",
              "  0.021954257041215897,\n",
              "  0.021908603608608246,\n",
              "  0.02185431309044361,\n",
              "  0.021779097616672516,\n",
              "  0.02178642340004444,\n",
              "  0.021724462509155273,\n",
              "  0.021800650283694267,\n",
              "  0.021590325981378555,\n",
              "  0.021532675251364708,\n",
              "  0.02174590341746807,\n",
              "  0.021319273859262466,\n",
              "  0.02142660692334175,\n",
              "  0.021209336817264557,\n",
              "  0.02117966301739216,\n",
              "  0.0210715439170599,\n",
              "  0.021248675882816315,\n",
              "  0.021034330129623413,\n",
              "  0.021133635193109512,\n",
              "  0.021062243729829788,\n",
              "  0.02091013640165329,\n",
              "  0.02084972709417343,\n",
              "  0.02102871797978878,\n",
              "  0.02073819749057293,\n",
              "  0.020679466426372528,\n",
              "  0.020592791959643364,\n",
              "  0.020706571638584137,\n",
              "  0.02063778042793274,\n",
              "  0.020488929003477097,\n",
              "  0.02049398608505726,\n",
              "  0.020397726446390152,\n",
              "  0.020299041643738747,\n",
              "  0.020299319177865982,\n",
              "  0.02069724164903164,\n",
              "  0.020136870443820953,\n",
              "  0.02023845911026001,\n",
              "  0.02005838043987751,\n",
              "  0.019996488466858864,\n",
              "  0.019918926060199738,\n",
              "  0.020000141113996506,\n",
              "  0.01989688165485859,\n",
              "  0.01999519392848015,\n",
              "  0.01993604190647602,\n",
              "  0.019750576466321945,\n",
              "  0.019847817718982697,\n",
              "  0.019735483452677727,\n",
              "  0.019680244848132133,\n",
              "  0.019566232338547707,\n",
              "  0.019567156210541725,\n",
              "  0.01956252194941044,\n",
              "  0.019653310999274254,\n",
              "  0.019546356052160263,\n",
              "  0.019534388557076454,\n",
              "  0.019566485658288002,\n",
              "  0.019807836040854454,\n",
              "  0.01929445005953312,\n",
              "  0.01963975466787815,\n",
              "  0.01930432766675949,\n",
              "  0.019219039008021355,\n",
              "  0.019123217090964317,\n",
              "  0.01912478171288967,\n",
              "  0.01920662261545658,\n",
              "  0.019230524078011513,\n",
              "  0.019160347059369087,\n",
              "  0.019005171954631805,\n",
              "  0.018970223143696785,\n",
              "  0.01892741210758686,\n",
              "  0.01895793341100216,\n",
              "  0.018931174650788307,\n",
              "  0.018842529505491257,\n",
              "  0.018789051100611687,\n",
              "  0.018808947876095772,\n",
              "  0.018788158893585205,\n",
              "  0.01869063824415207,\n",
              "  0.01866718754172325,\n",
              "  0.018829233944416046,\n",
              "  0.018550865352153778,\n",
              "  0.018676528707146645,\n",
              "  0.01853206381201744,\n",
              "  0.018442796543240547,\n",
              "  0.018470384180545807,\n",
              "  0.018507273867726326,\n",
              "  0.018368469551205635,\n",
              "  0.018365558236837387,\n",
              "  0.018408216536045074,\n",
              "  0.018364286050200462,\n",
              "  0.01829496957361698,\n",
              "  0.01838485710322857],\n",
              " 'val_accuracy': [0.8373786211013794,\n",
              "  0.8300970792770386,\n",
              "  0.8398058414459229,\n",
              "  0.8495145440101624,\n",
              "  0.8786407709121704,\n",
              "  0.8980582356452942,\n",
              "  0.9101941585540771,\n",
              "  0.9247573018074036,\n",
              "  0.9368932247161865,\n",
              "  0.9417475461959839,\n",
              "  0.9417475461959839,\n",
              "  0.9514563083648682,\n",
              "  0.9563106894493103,\n",
              "  0.9660193920135498,\n",
              "  0.9708737730979919,\n",
              "  0.9660193920135498,\n",
              "  0.9708737730979919,\n",
              "  0.9733009934425354,\n",
              "  0.9757281541824341,\n",
              "  0.9757281541824341,\n",
              "  0.9757281541824341,\n",
              "  0.9781553149223328,\n",
              "  0.9781553149223328,\n",
              "  0.9781553149223328,\n",
              "  0.9781553149223328,\n",
              "  0.9781553149223328,\n",
              "  0.9781553149223328,\n",
              "  0.9781553149223328,\n",
              "  0.9781553149223328,\n",
              "  0.9781553149223328,\n",
              "  0.9805825352668762,\n",
              "  0.9781553149223328,\n",
              "  0.9805825352668762,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9805825352668762,\n",
              "  0.9830096960067749,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9830096960067749,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.9902912378311157,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.9854369163513184,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217,\n",
              "  0.987864077091217],\n",
              " 'val_loss': [0.6846368908882141,\n",
              "  0.6711373925209045,\n",
              "  0.649211049079895,\n",
              "  0.6174404621124268,\n",
              "  0.5770940184593201,\n",
              "  0.5309051275253296,\n",
              "  0.48190173506736755,\n",
              "  0.4322811961174011,\n",
              "  0.38454023003578186,\n",
              "  0.3409716784954071,\n",
              "  0.301226943731308,\n",
              "  0.26633042097091675,\n",
              "  0.23649099469184875,\n",
              "  0.2107304185628891,\n",
              "  0.18867452442646027,\n",
              "  0.1703040599822998,\n",
              "  0.15379171073436737,\n",
              "  0.14063794910907745,\n",
              "  0.12969250977039337,\n",
              "  0.12020003795623779,\n",
              "  0.11181732267141342,\n",
              "  0.10460510849952698,\n",
              "  0.09850911051034927,\n",
              "  0.09326205402612686,\n",
              "  0.08852586150169373,\n",
              "  0.08466240763664246,\n",
              "  0.08031807094812393,\n",
              "  0.07711034268140793,\n",
              "  0.07396014779806137,\n",
              "  0.07131918519735336,\n",
              "  0.06872271001338959,\n",
              "  0.06681916862726212,\n",
              "  0.06469731777906418,\n",
              "  0.06333543360233307,\n",
              "  0.06147058680653572,\n",
              "  0.059159550815820694,\n",
              "  0.05758710205554962,\n",
              "  0.05661441385746002,\n",
              "  0.05535433441400528,\n",
              "  0.05432286486029625,\n",
              "  0.05309591069817543,\n",
              "  0.0516604445874691,\n",
              "  0.050673581659793854,\n",
              "  0.05014770105481148,\n",
              "  0.04941802844405174,\n",
              "  0.04874330013990402,\n",
              "  0.047581952065229416,\n",
              "  0.04706086590886116,\n",
              "  0.04648866876959801,\n",
              "  0.04584691673517227,\n",
              "  0.04512608051300049,\n",
              "  0.044413380324840546,\n",
              "  0.04381696134805679,\n",
              "  0.043254703283309937,\n",
              "  0.04305322840809822,\n",
              "  0.04235231131315231,\n",
              "  0.04198587313294411,\n",
              "  0.041399043053388596,\n",
              "  0.04092235863208771,\n",
              "  0.040446262806653976,\n",
              "  0.039844296872615814,\n",
              "  0.039381954818964005,\n",
              "  0.03989257290959358,\n",
              "  0.03928236663341522,\n",
              "  0.03856093809008598,\n",
              "  0.038218460977077484,\n",
              "  0.03812616690993309,\n",
              "  0.037583738565444946,\n",
              "  0.0373302586376667,\n",
              "  0.037214409559965134,\n",
              "  0.03729758784174919,\n",
              "  0.03690633550286293,\n",
              "  0.03613518178462982,\n",
              "  0.03633878380060196,\n",
              "  0.03566576540470123,\n",
              "  0.035295963287353516,\n",
              "  0.03509918600320816,\n",
              "  0.034348756074905396,\n",
              "  0.034448571503162384,\n",
              "  0.034405358135700226,\n",
              "  0.034394435584545135,\n",
              "  0.034823428839445114,\n",
              "  0.034017737954854965,\n",
              "  0.03467036038637161,\n",
              "  0.03422679379582405,\n",
              "  0.03417116403579712,\n",
              "  0.034057311713695526,\n",
              "  0.03345545381307602,\n",
              "  0.03314429521560669,\n",
              "  0.03299444913864136,\n",
              "  0.032614026218652725,\n",
              "  0.0327812060713768,\n",
              "  0.032814834266901016,\n",
              "  0.03249283507466316,\n",
              "  0.03238071873784065,\n",
              "  0.032376475632190704,\n",
              "  0.0329490564763546,\n",
              "  0.03249945864081383,\n",
              "  0.032191552221775055,\n",
              "  0.032374367117881775,\n",
              "  0.0322067029774189,\n",
              "  0.03127558156847954,\n",
              "  0.031316015869379044,\n",
              "  0.031531039625406265,\n",
              "  0.03150010481476784,\n",
              "  0.03132880851626396,\n",
              "  0.031245943158864975,\n",
              "  0.031193945556879044,\n",
              "  0.030787020921707153,\n",
              "  0.03022954612970352,\n",
              "  0.030682438984513283,\n",
              "  0.03071633167564869,\n",
              "  0.03047220967710018,\n",
              "  0.030490875244140625,\n",
              "  0.03021109662950039,\n",
              "  0.030178364366292953,\n",
              "  0.030206095427274704,\n",
              "  0.030221002176404,\n",
              "  0.029700374230742455,\n",
              "  0.030292360112071037,\n",
              "  0.029923850670456886,\n",
              "  0.02980378270149231,\n",
              "  0.0294259674847126,\n",
              "  0.029120003804564476,\n",
              "  0.029401274397969246,\n",
              "  0.029205825179815292,\n",
              "  0.029200149700045586,\n",
              "  0.029343729838728905,\n",
              "  0.02917909249663353,\n",
              "  0.02938610129058361,\n",
              "  0.02915927767753601,\n",
              "  0.02923296019434929,\n",
              "  0.02928098663687706,\n",
              "  0.029065588489174843,\n",
              "  0.028515726327896118,\n",
              "  0.029934462159872055,\n",
              "  0.029323304072022438,\n",
              "  0.028458096086978912,\n",
              "  0.028749387711286545,\n",
              "  0.029009902849793434,\n",
              "  0.029544945806264877,\n",
              "  0.029323872178792953,\n",
              "  0.030381737276911736,\n",
              "  0.0296159740537405,\n",
              "  0.02908979170024395,\n",
              "  0.02897617220878601,\n",
              "  0.028972266241908073,\n",
              "  0.02956240437924862,\n",
              "  0.028172263875603676,\n",
              "  0.029452009126544,\n",
              "  0.02875605598092079,\n",
              "  0.029256809502840042,\n",
              "  0.0287130456417799,\n",
              "  0.028648560866713524,\n",
              "  0.028329312801361084,\n",
              "  0.028217490762472153,\n",
              "  0.02816951833665371,\n",
              "  0.028600960969924927,\n",
              "  0.02811478264629841,\n",
              "  0.027446573600172997,\n",
              "  0.029325367882847786,\n",
              "  0.028677651658654213,\n",
              "  0.028497830033302307,\n",
              "  0.02914769947528839,\n",
              "  0.028937770053744316,\n",
              "  0.02851829305291176,\n",
              "  0.02846100740134716,\n",
              "  0.028389565646648407,\n",
              "  0.028211958706378937,\n",
              "  0.028207477182149887,\n",
              "  0.027976825833320618,\n",
              "  0.02828294038772583,\n",
              "  0.028398847207427025,\n",
              "  0.027966788038611412,\n",
              "  0.028165902942419052,\n",
              "  0.027686797082424164,\n",
              "  0.02801637537777424,\n",
              "  0.027808845043182373,\n",
              "  0.027861446142196655,\n",
              "  0.027954362332820892,\n",
              "  0.027475444599986076,\n",
              "  0.027933737263083458,\n",
              "  0.02756965346634388,\n",
              "  0.027799664065241814,\n",
              "  0.02844267711043358,\n",
              "  0.027739690616726875,\n",
              "  0.027722051367163658,\n",
              "  0.027797110378742218,\n",
              "  0.027739791199564934,\n",
              "  0.029235737398266792,\n",
              "  0.027948550879955292,\n",
              "  0.027674200013279915,\n",
              "  0.027908241376280785,\n",
              "  0.027583913877606392,\n",
              "  0.027933569625020027,\n",
              "  0.026662785559892654,\n",
              "  0.028132427483797073,\n",
              "  0.028466075658798218,\n",
              "  0.02778426744043827,\n",
              "  0.027577707543969154,\n",
              "  0.027813753113150597,\n",
              "  0.027755100280046463,\n",
              "  0.027728062123060226,\n",
              "  0.027134926989674568,\n",
              "  0.027543077245354652,\n",
              "  0.0274151973426342,\n",
              "  0.027891060337424278,\n",
              "  0.027469251304864883,\n",
              "  0.02754584513604641,\n",
              "  0.027596384286880493,\n",
              "  0.027360886335372925,\n",
              "  0.028607817366719246,\n",
              "  0.027751386165618896,\n",
              "  0.02897055633366108,\n",
              "  0.029611045494675636,\n",
              "  0.0283463466912508,\n",
              "  0.027149135246872902,\n",
              "  0.02876349166035652,\n",
              "  0.02941451407968998,\n",
              "  0.028760690242052078,\n",
              "  0.02809268608689308,\n",
              "  0.028049353510141373,\n",
              "  0.02799784578382969,\n",
              "  0.027026163414120674,\n",
              "  0.028166163712739944,\n",
              "  0.02757171355187893,\n",
              "  0.02793470397591591,\n",
              "  0.027364779263734818,\n",
              "  0.027712984010577202,\n",
              "  0.027303781360387802,\n",
              "  0.02813176065683365,\n",
              "  0.027867956086993217,\n",
              "  0.027883322909474373,\n",
              "  0.029375210404396057,\n",
              "  0.028261467814445496,\n",
              "  0.02780667319893837,\n",
              "  0.028023522347211838,\n",
              "  0.028064802289009094,\n",
              "  0.027278224006295204,\n",
              "  0.027399441227316856,\n",
              "  0.027146026492118835,\n",
              "  0.02722691185772419,\n",
              "  0.027568481862545013,\n",
              "  0.027580851688981056,\n",
              "  0.027308329939842224,\n",
              "  0.02739899419248104,\n",
              "  0.027750790119171143,\n",
              "  0.027291975915431976,\n",
              "  0.027867000550031662,\n",
              "  0.02678586356341839]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1dCtabMx-f3"
      },
      "source": [
        "# training loss versus validation loss plot "
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "g8OdvYhns20h",
        "outputId": "be486fc3-c0f9-47c4-be14-9e8706b42fa7"
      },
      "source": [
        "#Python Dictionary with 4 keys accuracy, loss (training), val_loss, val_accuracy\n",
        "plt.plot(range(num_epochs-1), history.history['loss'][1:], label='train_loss')\n",
        "plt.plot(range(num_epochs-1), history.history['val_loss'][:-1], label='test_loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.title('training loss v/s test loss')\n",
        "plt.show()\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "MCPTC3ZSs28c",
        "outputId": "53332625-7da7-4f29-aa23-8b203a62e6ee"
      },
      "source": [
        "plt.plot(range(num_epochs-1), history.history['accuracy'][1:], label='train_acc')\n",
        "plt.plot(range(num_epochs-1), history.history['val_accuracy'][:-1], label='test_acc')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.title('training accuracy v/s test accuracy')\n",
        "plt.show()\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cODMYGwEx44n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673d31e9-aadb-46ef-f729-9a06cece8b34"
      },
      "source": [
        "print('Minimum Validation Loss:', np.min(history.history['val_loss']))\n",
        "print('Best epoch corresponding to Minimum Validation Loss:', np.argmin(history.history['val_loss'])+1)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum Validation Loss: 0.026662785559892654\n",
            "Best epoch corresponding to Minimum Validation Loss: 196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51rb_97Kx5da"
      },
      "source": [
        "model_best = load_model('/content/saved_models/model-00222.h5')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i13hEgLx2Mc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574483ed-b160-4768-9adc-2d3aa7c16cfc"
      },
      "source": [
        "print('AUC on test =', roc_auc_score(y_true=testY, y_score=model_best.predict_proba(testX_scaled)))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC on test = 0.9997375140191377\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B8fh0w7s3GK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d30aeb-f77b-4136-9f2a-8dac8e5fd9a1"
      },
      "source": [
        "print('AUC on train =', roc_auc_score(y_true=trainY, y_score=model_best.predict_proba(trainX_scaled)))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC on train = 0.9999209107565764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLw_2x7oPNBy",
        "outputId": "58d16ca6-1206-41b6-9db5-005414468483"
      },
      "source": [
        "y_pred= model_best.predict_classes(testX_scaled)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOJzvSRdOxc2"
      },
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Mt56YWOOxi-",
        "outputId": "cc143dd3-978e-4c4d-a9fe-d5d7d8a2ec97"
      },
      "source": [
        "# ('F1 score')=# f1: 2 tp / (2 tp + fp + fn)\n",
        "## F-score or F-measure is a measure of a test's accuracy.\n",
        "f1 = f1_score(testY, y_pred)\n",
        "print('F1 score: %f' % f1)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score: 0.986226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBMDoFfHRiBK"
      },
      "source": [
        ""
      ],
      "execution_count": 61,
      "outputs": []
    }
  ]
}